{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas numpy matplotlib seaborn rasterio tqdm pyproj shapely scikit-learn tensorflow==2.10.0\n",
    "#! conda install -c conda-forge libgdal -y\n",
    "#! conda install -c conda-forge gdal -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raster manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "from rasterio import warp, mask, features\n",
    "from rasterio.windows import Window\n",
    "from rasterio.plot import show, reshape_as_image, adjust_band, reshape_as_raster\n",
    "from osgeo import gdal, ogr, osr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "from pyproj import Proj, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.controllers.raster import RasterController\n",
    "from src.controllers.pixel import PixelController\n",
    "from src.controllers.tile import TileController\n",
    "from src.layers.dict_class import LandCoverClassDict\n",
    "from src.layers.utilities import process_data_points, locations_of_pixels, extract_landsat_images, read_metadata, search_mtl_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove folder:  tmpvghso4p2 Product:  LC08_L1TP_010063_20170819_20200903_02_T1\n",
      "Remove folder:  tmpqk8a0xji Product:  LC08_L1TP_010063_20210526_20210529_02_T1\n",
      "Remove folder:  tmpddbzlvr_ Product:  LC08_L1TP_010063_20180907_20200831_02_T1\n",
      "Remove folder:  tmppg2_o9zs Product:  LC08_L1TP_010063_20180619_20200831_02_T1\n",
      "Remove folder:  tmpbu9r3606 Product:  LC08_L1TP_010063_20220614_20220617_02_T1\n",
      "Remove folder:  tmpk4fiu7ke Product:  LC08_L1TP_010063_20220902_20220913_02_T1\n",
      "Remove folder:  tmp_r3ob753 Product:  LC08_L1TP_010063_20141030_20200910_02_T1\n",
      "Remove folder:  tmpcmtg2cl9 Product:  LC08_L1TP_010063_20211001_20211013_02_T1\n",
      "Remove folder:  tmpyu2tci4k Product:  LC08_L1TP_010063_20190825_20200826_02_T1\n",
      "Remove folder:  tmpdobwkm2s Product:  LC08_L1TP_010063_20200912_20200919_02_T1\n",
      "Remove folder:  tmpayq4amvx Product:  LC08_L1TP_010063_20130808_20200912_02_T1\n",
      "Remove folder:  tmpq29nq6s1 Product:  LC08_L1TP_010063_20150713_20200908_02_T1\n",
      "Remove folder:  tmpn1k_frto Product:  LC08_L1TP_010063_20131128_20200912_02_T1\n",
      "Remove folder:  tmptjsw8r7g Product:  LC08_L1TP_010063_20200811_20200918_02_T1\n",
      "Remove folder:  tmpb5wm1v6m Product:  LC08_L1TP_010063_20150915_20200908_02_T1\n",
      "Remove folder:  tmppm0shc_k Product:  LC08_L1TP_010063_20151102_20210219_02_T1\n",
      "Remove folder:  tmpceu411fv Product:  LC08_L1TP_010063_20200827_20200906_02_T1\n",
      "Remove folder:  tmpp3mzz4xi Product:  LC08_L1TP_010063_20190926_20200825_02_T1\n",
      "Remove folder:  tmp89zib8z5 Product:  LC08_L1TP_010063_20170920_20200903_02_T1\n",
      "Remove folder:  tmpcckk9oto Product:  LC08_L1TP_010063_20210627_20210707_02_T1\n",
      "Remove folder:  tmp42gs_4c1 Product:  LC08_L1TP_010063_20190606_20200828_02_T1\n",
      "Remove folder:  tmpdywzxxiz Product:  LC08_L1TP_010063_20151001_20200908_02_T1\n",
      "Remove folder:  tmpav7hy9st Product:  LC08_L1TP_010063_20161120_20200905_02_T1\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "mask = '../data/shape/Loja/Loja.shp'\n",
    "data_tar = os.listdir('../data/images/raw/products/')\n",
    "bands_list = []\n",
    "\n",
    "for x in data_tar:\n",
    "    bands, mtl = extract_landsat_images(f'../data/images/raw/products/{x}')\n",
    "    metadata = read_metadata(mtl)\n",
    "    product_id = search_mtl_params('LANDSAT_PRODUCT_ID', metadata)\n",
    "    merge = RasterController().merge_rasters(bands, product_id)\n",
    "    crop = RasterController().crop_mask_raster(merge, mask)\n",
    "    toa = RasterController().correct_toa_radiance(crop, metadata)\n",
    "    tmp = mtl.split('/')[3]\n",
    "    shutil.rmtree(f'/tmp/landsat/{tmp}')\n",
    "    print('Remove folder: ', tmp, 'Product: ', product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed\n",
    "RANDOM_SEED = 21\n",
    "\n",
    "# Set random state for Python\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "# Set random state for Python random function\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set random state for NumPy\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set random state for TensorFlow\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info():\n",
    "    tf_version = tf.__version__\n",
    "    gpus = [x.physical_device_desc for x in device_lib.list_local_devices() if x.device_type == 'GPU']\n",
    "    cuda_version = tf.sysconfig.get_build_info()['cuda_version']\n",
    "    cudnn_version = tf.sysconfig.get_build_info()['cudnn_version']\n",
    "    \n",
    "    print('Versión de TensorFlow: {}'.format(tf_version))\n",
    "    print('GPU: {}'.format(gpus))\n",
    "    print('Versión Cuda: {}'.format(cuda_version))\n",
    "    print('Versión Cudnn: {}\\n'.format(cudnn_version))\n",
    "\n",
    "print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tile_generator(l8_image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size, merge=False):\n",
    "    \"\"\"\n",
    "    Generador de mosaicos para tomar esas localizaciones de píxeles y construir mosaicos del formato adecuado. \n",
    "    Este generador se entrega directamente al modelo `keras` y alimenta continuamente de datos al modelo durante el entrenamiento y la validación.\n",
    "    \"\"\"\n",
    "    # this is a keras compatible data generator which generates data and labels on the fly\n",
    "    # from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "\n",
    "    label_proj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    l8_band_count = l8_image_datasets[0].count\n",
    "    band_count = l8_band_count\n",
    "    class_count = len(class_names)\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "\n",
    "    while True:\n",
    "        # take one off because we don't want the QA band\n",
    "        image_batch = np.zeros(\n",
    "            (batch_size, tile_height, tile_width, band_count-1))\n",
    "        label_batch = np.zeros((batch_size, class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i = 0\n",
    "            r, c = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = l8_image_datasets[dataset_index].read(list(np.arange(\n",
    "                1, l8_band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if tile.size == 0:\n",
    "                pass\n",
    "            elif np.amax(tile) == 0:  # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile:\n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            elif tile.shape != (l8_band_count, tile_width, tile_height):\n",
    "                # print('wrong shape')\n",
    "                # print(tile.shape)\n",
    "                # somehow we're randomly getting tiles without the correct dimensions\n",
    "                pass\n",
    "            elif np.isin(tile[7, :, :], [352, 368, 392, 416, 432, 480, 840, 864, 880, 904, 928, 944, 1352]).any() == True:\n",
    "                # make sure pixel doesn't contain clouds\n",
    "                # this is probably pretty inefficient but only checking width x height for each tile\n",
    "                # read more here: https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1873_US_Landsat_ARD_DFCB_0.pdf\n",
    "                # print('Found some cloud.')\n",
    "                # print(tile[7,:,:])\n",
    "                pass\n",
    "            else:\n",
    "                # taking off the QA band\n",
    "                tile = tile[0:7]\n",
    "                # reshape from raster format to image format and standardize according to image wide stats\n",
    "                reshaped_tile = (reshape_as_image(tile) - 982.5) / 1076.5\n",
    "\n",
    "                # get label data\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = l8_image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                if l8_proj != label_proj:\n",
    "                    transformer = Transformer.from_crs(l8_proj.srs, label_proj.srs, always_xy=True)\n",
    "                    x, y = transformer.transform(x, y)\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x, y)\n",
    "\n",
    "                # find label\n",
    "                # label image could be huge so we need this to just get a single position\n",
    "                window = ((row, row+1), (col, col+1))\n",
    "                data = LandCoverClassDict().merge_classes(label_dataset.read(\n",
    "                    1, window=window, masked=False, boundless=True), )\n",
    "                label = data[0, 0]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary of all classes and class identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = LandCoverClassDict().get_landsat_dictionary()\n",
    "colors = LandCoverClassDict().get_colors_dictionary()\n",
    "\n",
    "print(\"class landcover:\", class_names, sep=\"\\n\")\n",
    "print(\"colors:\", colors, sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raster exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = '../data/images/processed/labels/landsat/landcover_ecuador_v1.tif'\n",
    "rasters_path = '../data/images/processed/products/L8/LC08_L1TP_010063_20141030_20200910_02.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_controller = RasterController()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = raster_controller.open_raster(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_image = raster_controller.read_raster(dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(labels_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_rasters =  [rio.open(f'{rasters_path}/{raster}') for raster in os.listdir(rasters_path)]\n",
    "#print(\"Numero de Rasters de entrenamiento: \", len(dataset_rasters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_dataser = raster_controller.open_raster(rasters_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate balanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_controller = PixelController(label_dataset=dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pixels = pixel_controller.generate_training_pixels(image_datasets=[raster_dataser], train_count=500, merge=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation\n",
    "- Balanced and accurate dataset.\n",
    "- Actual pixel locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_locations = process_data_points(train_pixels=train_pixels, landsat_datasets=[raster_dataser], dataset_labels=dataset_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Data Locations\n",
    "Now display the class map along with the locations of training pixels on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_of_pixels(labels_image, label_locations, colors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Data Generator\n",
    "Now let's test the data generator to ensure it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_proj = Proj(raster_dataser.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"/home/theend/Documentos/Develop/Python/Jupyter/soil_desertification/src/controllers/tile.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_controller = TileController(tile_height=128, tile_width=128, pixel_locations=train_pixels, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_batch = None\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_controller.tile_generators(images_datasets=[raster_dataser], label_dataset=dataset_labels):\n",
    "    if count > 3:\n",
    "        break\n",
    "    print('Image')\n",
    "    print(im.shape)\n",
    "    print('Label')\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im\n",
    "    label_batch = label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los mosaicos\n",
    "\n",
    "Ahora vamos a visualizar los azulejos reales. Tenga en cuenta que se verán poco naturales porque han sido normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_labels(image_batch, label_batch, class_names):\n",
    "    num_images = image_batch.shape[0]\n",
    "    num_cols = 3\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(1.62 * 10, 10))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        axes[row, col].imshow(normalize(image_batch[i, :, :, 3:6]))\n",
    "        axes[row, col].set_title(class_names[np.argmax(label_batch[i])])\n",
    "\n",
    "    # Remove any empty subplots\n",
    "    for i in range(num_images, num_rows * num_cols):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def normalize(image):\n",
    "    return (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_with_labels(im_batch, label_batch, LandCoverClassDict().get_landsat_dictionary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 1x1 tile training dataset for scikit-learn for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_batch = None\n",
    "label_batch = None\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_generator(dataset_rasters, dataset_labels, 1, 1, train_pixels, sample_size):\n",
    "    if count > 0:\n",
    "        break\n",
    "    print('Batch Shape')\n",
    "    print(im.shape)\n",
    "    print('Label Shape')\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im\n",
    "    label_batch = label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize\n",
    "Reshape because scikit-learn requires data in the `(samples, features)` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_batch[0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_batch_reshaped = im_batch.reshape(sample_size, 7)\n",
    "im_batch_reshaped[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Spectral Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=[10, 10])\n",
    "\n",
    "# numbers 1-8\n",
    "band_count = np.arange(1, 8)\n",
    "\n",
    "y = np.argmax(label_batch, axis=1)\n",
    "X = im_batch_reshaped\n",
    "\n",
    "classes = np.unique(y)\n",
    "for class_type in classes:\n",
    "    band_intensity = np.mean(X[y == class_type, :], axis=0)\n",
    "    ax.plot(band_count, band_intensity, label=class_names[class_type])\n",
    "# plot them as lines\n",
    "\n",
    "# Add some axis labels\n",
    "ax.set_xlabel('Band #')\n",
    "ax.set_ylabel('Reflectance Value')\n",
    "# Add a titleA\n",
    "ax.set_title('Band Intensities Full Overview')\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Actual Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras    \n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, Dropout\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "epochs = 25\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# dimensiones de la imagen de entrada\n",
    "tile_side = 32\n",
    "img_rows, img_cols = tile_side, tile_side\n",
    "img_bands = landsat_datasets[0].count - 1\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1\n",
    "l1 = tf.keras.regularizers.l1(0)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', activity_regularizer=l1))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activity_regularizer=l1, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same',activity_regularizer=l1, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activity_regularizer=l1, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activity_regularizer=l1, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3200, activation='relu'))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Bidirectional, LSTM, Dense, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Hiperparámetros\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 50  # Puedes ajustar esto según el rendimiento y el uso de la detección temprana.\n",
    "dropout_rate = 0.3  # Puedes ajustar esto según el rendimiento.\n",
    "\n",
    "# Creación del modelo\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Agregamos una capa de Reshape para convertir la salida en 3D\n",
    "model.add(Reshape((1, 1024)))\n",
    "\n",
    "# Agregamos dos capas Bidirectional LSTM\n",
    "model.add(Bidirectional(LSTM(128, activation='relu', return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, activation='relu')))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compilación del modelo\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics and Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=Adam(lr=0.0003), metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_val_ratio = 0.8\n",
    "train_px = train_pixels[:int(len(train_pixels)*train_to_val_ratio)]\n",
    "val_px = train_pixels[int(len(train_pixels)*train_to_val_ratio):]\n",
    "print(\"# Training samples: {n_training} \\n# Validation samples: {n_val}\".format(n_training=len(train_px), n_val=len(val_px)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.95):\n",
    "            print(\"\\n Se ha alcanzado el umbral PRESICION. Entrenamiento detenido.....\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Let's create an object of our class and assign it to a variable\n",
    "early_stopping = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../reports/logs/landsat')\n",
    "\n",
    "history = model.fit(\n",
    "    tile_generator(\n",
    "        landsat_datasets,\n",
    "        dataset_labels,\n",
    "        tile_side,\n",
    "        tile_side,\n",
    "        train_px,\n",
    "        batch_size, merge=True\n",
    "    ),\n",
    "    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "    validation_data=tile_generator(\n",
    "        landsat_datasets,\n",
    "        dataset_labels,\n",
    "        tile_side,\n",
    "        tile_side,\n",
    "        val_px,\n",
    "        batch_size,\n",
    "        merge=True),\n",
    "    validation_steps=len(val_px) // batch_size,\n",
    "    callbacks=[early_stopping, tensorboard]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../models/landsat/'\n",
    "model.save(f\"{save_path}model_v1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Training and Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.62*7, 7))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.62*7, 7))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model error\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "_ = plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Test Accuracy Based on Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(\n",
    "    tile_generator(\n",
    "        landsat_datasets,\n",
    "        dataset_labels,\n",
    "        tile_side,\n",
    "        tile_side,\n",
    "        val_px,\n",
    "        batch_size,\n",
    "        merge=True\n",
    "    ),\n",
    "    steps=len(val_px) // batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator(\n",
    "    landsat_datasets,\n",
    "    dataset_labels,\n",
    "    tile_side,\n",
    "    tile_side,\n",
    "    val_px,\n",
    "    batch_size=1,\n",
    "    merge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(labels):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1\n",
    "\n",
    "label_index = np.argmax(labels, axis=1)\n",
    "pred_index = np.argmax(predictions, axis=1)\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Unnormalized Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    label_index,\n",
    "    pred_index,\n",
    "    classes=np.array(list(class_names)),\n",
    "    class_dict=class_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Normalized Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(\n",
    "    label_index,\n",
    "    pred_index,\n",
    "    classes=np.array(list(class_names)),\n",
    "    class_dict=class_names,\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {accuracy:.2f}%\".format(accuracy=accuracy_score(label_index, pred_index)*100))\n",
    "print(\"Precision: {precision:.2f}%\".format(precision=precision_score(label_index, pred_index, average='weighted')*100))\n",
    "print(\"Recall: {recall:.2f}%\".format(recall=recall_score(label_index, pred_index, average='weighted')*100))\n",
    "print(\"F1 score: {f1:.2f}%\".format(f1=f1_score(label_index, pred_index, average='weighted')*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Training Accuracy Based on Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(\n",
    "    tile_generator(\n",
    "        landsat_datasets,\n",
    "        dataset_labels,\n",
    "        tile_side,\n",
    "        tile_side,\n",
    "        train_px,\n",
    "        batch_size,\n",
    "        merge=True\n",
    "    ),\n",
    "    steps=len(train_px) // batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator(\n",
    "    landsat_datasets,\n",
    "    dataset_labels,\n",
    "    tile_side,\n",
    "    tile_side,\n",
    "    train_px,\n",
    "    batch_size=1,\n",
    "    merge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(labels):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1\n",
    "\n",
    "label_index = np.argmax(labels, axis=1)\n",
    "pred_index = np.argmax(predictions, axis=1)\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Unnormalized Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(\n",
    "    label_index,\n",
    "    pred_index,\n",
    "    class_dict=class_names,\n",
    "    title='Confusion matrix - Unnormalized'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Normalized Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(\n",
    "    label_index,\n",
    "    pred_index,\n",
    "    class_dict=class_names,\n",
    "    title='Confusion matrix - Normalized',\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {accuracy:.2f}%\".format(accuracy=accuracy_score(label_index, pred_index)*100))\n",
    "print(\"Precision: {precision:.2f}%\".format(precision=precision_score(label_index, pred_index, average='weighted')*100))\n",
    "print(\"Recall: {recall:.2f}%\".format(recall=recall_score(label_index, pred_index, average='weighted')*100))\n",
    "print(\"F1 score: {f1:.2f}%\".format(f1=f1_score(label_index, pred_index, average='weighted')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IPython Notebook execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('../models/landsat/model_v1.h5')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soil-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
